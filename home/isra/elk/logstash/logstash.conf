input {
  # JSON files (JSONL format - one JSON per line)
  file {
    path => "/logs/*.json"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => "json_lines"
    type => "json"
  }
  
  # TXT files
  file {
    path => "/logs/*.txt"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    type => "txt"
  }
  
  # LOG files
  file {
    path => "/logs/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    type => "log"
  }
  
  # CSV files
  file {
    path => "/logs/*.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    type => "csv"
  }
}

filter {
  # For JSON files - already parsed by json_lines codec
  if [type] == "json" {
    # Data is already in JSON format, just add metadata
    mutate {
      add_field => { "indexed_at" => "%{@timestamp}" }
    }
  }
  
  # For TXT files - try JSON first, then key=value
  else if [type] == "txt" {
    # Try to parse as JSON
    if [message] =~ /^{.*}$/ {
      json {
        source => "message"
        skip_on_invalid_json => true
      }
      mutate {
        remove_field => ["message"]
      }
    }
    # Otherwise try key=value format
    else {
      grok {
        match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:level} %{GREEDYDATA:data}" }
      }
      if [data] {
        kv {
          source => "data"
          field_split => " "
          value_split => "="
        }
        mutate {
          remove_field => ["data", "message"]
        }
      }
    }
  }
  
  # For LOG files - grok pattern
  else if [type] == "log" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:level} %{WORD:event} %{GREEDYDATA:data}"
      }
    }
    
    if [data] {
      kv {
        source => "data"
        field_split => " "
        value_split => "="
      }
      mutate {
        remove_field => ["data", "message"]
      }
    }
  }
  
  # For CSV files
  else if [type] == "csv" {
    csv {
      skip_header => true
      separator => ","
      columns => ["timestamp","level","service","transaction_id","user_id","amount","currency","status","method"]
    }
  }
  
  # Convert timestamp
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss'Z'"]
      target => "@timestamp"
      remove_field => ["timestamp"]
    }
  }
  
  # Type conversions for e-commerce data
  if [amount] {
    mutate { convert => { "amount" => "float" } }
  }
  
  if [user_id] {
    mutate { convert => { "user_id" => "integer" } }
  }
  
  if [risk_score] {
    mutate { convert => { "risk_score" => "integer" } }
  }
  
  # Clean up
  mutate {
    remove_field => ["host"]
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "logs-ecommerce-%{+YYYY.MM.dd
        match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:level} %{GREEDYDATA:data}" }
      }
      if [data] {
        kv {
          source => "data"
          field_split => " "
          value_split => "="
        }
        mutate {
          remove_field => ["data", "message"]
        }
      }
    }
  }
  
  # For LOG files - grok pattern
  else if [type] == "log" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:level} %{WORD:event} %{GREEDYDATA:data}"
      }
    }
    
    if [data] {
      kv {
        source => "data"
        field_split => " "
        value_split => "="
      }
      mutate {
        remove_field => ["data", "message"]
      }
    }
  }
  
  # For CSV files
  else if [type] == "csv" {
    csv {
      skip_header => true
      separator => ","
      columns => ["timestamp","level","service","transaction_id","user_id","amount","currency","status","method"]
    }
  }
  
  # Convert timestamp
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss'Z'"]
      target => "@timestamp"
      remove_field => ["timestamp"]
    }
  }
  
  # Type conversions for e-commerce data
  if [amount] {
    mutate { convert => { "amount" => "float" } }
  }
  
  if [user_id] {
    mutate { convert => { "user_id" => "integer" } }
  }
  
  if [risk_score] {
    mutate { convert => { "risk_score" => "integer" } }
  }
  
  # Clean up
  mutate {
    remove_field => ["host"]
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "logs-ecommerce-%{+YYYY.MM.dd}"
  }
  stdout { codec => rubydebug }
}
